# -*- coding: utf-8 -*-
"""lab4_文字探勘.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_aNQoSuDy8vly4c8FeFTEh9HBdrhoPOR
"""

!pip install transformers

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from transformers import AdamW

import numpy as np
import pandas as pd

df = pd.read_csv("/content/drive/Shareddrives/colab/IMDB Dataset.csv")
df.head()

df.describe(include='all')

df = df.sample(frac=0.2, random_state=42)
reviews = df["review"].to_numpy()

labels = df['sentiment'].map({"positive": 1, "negative": 0}).to_numpy()

labels

X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=42)

X_train[0], y_train[0]

"""初始化 BERT 模型和分詞器:"""

# 初始化BERT模型和tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

"""將電影評論轉換為適合BERT模型輸入的形式"""

# 定義函數將電影評論轉換為 BERT 輸入格式
# 這包括將評論轉換為 input IDs 和 attention masks
def tokenize_reviews(reviews, labels):
    ...
def tokenize_reviews(reviews, labels):
  input_ids = [] # convert tokens to integers (each id represent a unique token) 用於存儲每個電影評論文本經過分詞後的標記（token）整數表示

# The purpose of the attention mask is to handle sequences of varying lengths.
# In BERT, input sequences are padded or truncated to a fixed length,
# and the attention mask helps the model know which parts are actual data and which are padding.
# e.g. ["I","love","NLP"] -> [1,1,1,0,0] with fixed length 5
  attention_masks = []   # 在BERT中，輸入序列被填充或截斷到一個固定的長度，attention_masks幫助模型知道哪些部分是實際數據，哪些部分是填充（padding）。

  for review in reviews:
    # 使用BERT的分詞器tokenizer來對文本進行編碼和轉換，以便將其轉化為適合BERT輸入的格式
    encoded_dict = tokenizer.encode_plus(    # encode_plus接受文本、標記特殊標記（如[CLS]和[SEP]），以及其他參數，例如最大長度、填充（padding）和截斷
      review,
      add_special_tokens=True, # [CLS]：開始分類的特殊標記、[SEP]：分隔符、[PAD]：填充（Padding）。输入序列通常需要具有相同的长度，但真实的文本序列可能具有不同的长度、[UNK]：未知（Unknown）
      max_length=128,
      padding='max_length',
      truncation=True,
      return_attention_mask=True,
      return_tensors='pt',
    )
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict[ 'attention_mask'])

  # 將input_ids、attention_masks和labels轉換為PyTorch張量（tensor），以便進行深度學習模型的訓練
  input_ids = torch.cat(input_ids, dim=0)
  attention_masks = torch.cat(attention_masks, dim=0)
  labels = torch.tensor(labels)
  return input_ids, attention_masks, labels

# Convert data to PyTorch tensors
input_ids, attention_masks, labels = tokenize_reviews(X_train, y_train)

# 創建 DataLoader 用於加載訓練數據
dataset = TensorDataset(input_ids, attention_masks, labels)  # TensorDataset是PyTorch提供的一個用於包裝張量數據的工具，以便更容易地進行批次處理
train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  # DataLoader用於加載訓練數據批次  # shuffle=True表示在每個訓練周期（epoch）開始時打亂數據，以確保模型在不同的小批次上訓練

"""設置優化器、損失函數和訓練週期"""

# 設置AdamW優化器，學習率為2e-5
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
# 使用交叉熵損失函數
loss_fn = torch.nn.CrossEntropyLoss ()
# 設置訓練週期為2
epochs = 2

# 檢查是否有CUDA可用
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# 將模型移動到該設備上
model.to(device)

train_loss_set = []

"""訓練bert模型"""

# 進行訓練
for epoch in range(epochs):
  # set to training mode
  # cuz some layers (dropout, batch norm) perform differently during training
  model.train()
  for batch in train_dataloader:
    input_ids, attention_mask, labels = batch
    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
    # call at each new batch
    # PyTorch accumulates gradients by default,
    # and we want to compute fresh gradients for each batch.
    optimizer.zero_grad()
    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
    loss = outputs.loss
    train_loss_set.append(loss.item())
    loss.backward() # compute the gradient
    optimizer.step() # update parameters (optimization)

"""評估訓練好的模型"""

# 設置模型為評估模式
model.eval()
# 使用測試數據進行評估
test_input_ids, test_attention_masks, test_labels = tokenize_reviews(X_test, y_test)
test_input_ids, test_attention_masks, test_labels = test_input_ids.to(device), test_attention_masks.to(device), test_labels.to(device)

with torch.no_grad():
  logits = model(test_input_ids, attention_mask=test_attention_masks)

predicted_labels = np.argmax(logits.logits.cpu().numpy(),axis=1)
accuracy = accuracy_score(y_test, predicted_labels)
# 計算並打印準確率
print(f'Accuracy: {accuracy:.2f}')

"""
1. 初始化裝置
首先，我們清空了GPU上的記憶體，這有助於避免連續運行模型時出現記憶體不足的問題。然後，我們確定了模型所在的裝置，這可能是CPU或GPU。

2. 數據準備
從資料框中提取所有電影評論，然後選擇前5條評論來進行推論。接著，我們將情感標籤從文字轉換為數值形式。

3. 模型評估
將模型設置為評估模式，這意味著模型在前向傳播時不會保存任何用於反向傳播的資訊。
4. 數據轉換
使用先前定義的tokenize_reviews函數對選定的評論進行分詞和編碼。

5. 模型推論
使用模型對輸入數據進行推論，獲取模型的輸出。

6. 概率轉換
將模型的輸出轉換為概率形式，這有助於解釋模型的預測

"""

# 清空GPU上的記憶體，這有助於避免在連續執行模型時發生記憶體不足的問題
torch.cuda.empty_cache()

# 獲取模型的裝置資訊 (可能是CPU或GPU)
device = next(model.parameters()).device

# 從資料框中獲取所有的電影評論
reviews = df["review"].to_numpy()

# 選取前5條評論進行推論
inf_reviews =reviews[:5]

# 將情感標籤從文字轉換為數值形式
labels = df['sentiment'].map({"positive": 1, "negative": 0}).to_numpy()
# 選取對應於前5條評論的標籤
inf_labels = labels[:5]
print(inf_labels)

# 將模型設置為評估模式
model.eval()

# 使用先前定義的tokenize_reviews函數對評論進行分詞和編碼
inf_inp_ids, inf_att_masks, inf_labels = tokenize_reviews(inf_reviews, inf_labels)
# 將數據移動到模型所在的裝置上
inf_inp_ids, inf_att_masks, inf_labels = inf_inp_ids.to(device), inf_att_masks.to(device), inf_labels.to(device)

# 進行模型推論，不計算梯度以提高效率
with torch.no_grad():
    inf_logits = model(inf_inp_ids, attention_mask=inf_att_masks)

# 使用sigmoid函數將logits轉換為概率
probabilities = torch.sigmoid(logits)

print(logits)

"""印出評論和bert所預測的概率（右邊為positive的概率左邊為negative的概率）"""

for i in range(5):
  print(f"""
  review: {inf_reviews[i]}
  predict: {probabilities[i]}
  ground truth: {inf_labels[i]}
  ------------------------------
  """)

"""## 分析模型表現"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score

print(classification_report(y_test, predicted_labels, target_names = ['Bad Reviews','Good Reviews']))

"""混淆矩陣"""

cm = confusion_matrix(y_test,predicted_labels)
cm

plt.figure(figsize = (10,10))
sns.heatmap(cm,cmap= "Blues", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Bad Reviews','Good Reviews'] , yticklabels = ['Bad Reviews','Good Reviews'])
plt.xlabel("Predicted")
plt.ylabel("Actual")

"""觀察training loss的變動"""

# plot training performance
plt.figure(figsize=(15,8))
plt.title("Training loss")
plt.xlabel("Batch")
plt.ylabel("Loss")
plt.plot(train_loss_set)
plt.show()

"""通常模型優化後，training loss會逐漸下降。如果training loss在訓練過程中出現波動或上升，可能需要進一步分析和優化模型。"""